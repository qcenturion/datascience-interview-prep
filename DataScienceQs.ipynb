{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running list of data science interview questions/answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How should outliers be detected and handled?\n",
    "\n",
    "One method is to use a box and whisker plot, and define outliers to be data points outside of Q1 - 1.5IQR and Q3 + 1.5IQR\n",
    "\n",
    "#### Explain Frequentist vs Bayesian approaches to statistical analysis\n",
    "\n",
    "The frequentist approach hinges on the belief of long-run probability, and attempting to make statements about some true (but unknown) parameters in the distribution that generated the sample data. \n",
    "The Bayesian approach uses some a-priori belief, and updates that belief with data, to make statements about the degree to which they believe something to be true. \n",
    "\n",
    "#### Explain the concept of a p-value in hypothesis testing\n",
    "\n",
    "P-values or probability values measure the probability of observing sample data as or more extreme than what was observed, under a true null hypothesis. A decision rule is formed using the significance level, alpha (type 1 error rate), and the p-value is compared to alpha to make a decision about rejecting or failing to reject a null hypothesis. \n",
    "\n",
    "#### What are type I and type II error rates? What is power? Which is more important?\n",
    "\n",
    "#### Explain bar chart vs histogram\n",
    "\n",
    "#### Explain the key assumptions for linear regression\n",
    "\n",
    "#### What is R squared?\n",
    "\n",
    "#### Confidence vs Prediction interval*\n",
    "\n",
    "Once a regression equation has been obtained, we may wish to predict a response $\\hat{y}$ for new data $x_{new}$. The regression equation is\n",
    "\n",
    "$$\\large{\\mathbf{y=X^{T}\\beta+\\epsilon}}$$\n",
    "\n",
    "Here, $\\mathbf{X}$ is the design matrix, $\\mathbf{\\beta}$ is the vector of parameters, and $\\mathbf{\\epsilon}$ is the error vector (assumed to follow $\\epsilon_{i}~$~$^{iid}~N(0,\\sigma^{2})$). For $\\mathbf{x_{new}}$ the predicted response will be\n",
    "\n",
    "$$\\hat{y}=x_{new}^{T}\\hat{\\beta}$$\n",
    "\n",
    "where $\\hat{\\beta}$ is the vector of predicted coefficients for each explanatory variable. Of course we don't expect our prediction to be exactly equal to the true value; there is uncertainty in our coefficient estimates and also the model itself; even if we had $\\hat{\\beta}=\\beta$ we would have uncertainty from $\\epsilon$. If we want to form a confidence interval for the true mean value of predictions made on particular new data $x_{new}$, we would do so by using $\\hat{y}$ as our point estimate, and building the interval around it. We seek an interval for the expected value of $y$ given new data; $\\mathbb{E}[{y}~|~x_{new}]$. Thus we must add and subtract from our point estimate a margin of error, which depends on the degree of confidence desired for the interval. Note that\n",
    "\n",
    "$$\\mathbb{E}[\\hat{y}~|~x_{new}]~=~\\mathbb{E}[~x_{new}^{T}\\hat{\\beta}+\\epsilon~]~=~x_{new}^{T}\\beta$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explain precision vs accuracy in binary classification\n",
    "\n",
    "Accuracy is the proportion of correct predictions $\\frac{TP + TN}{Total}$, and precision is $\\frac{TP}{TP + FP}$, where TP is True Positive etc. In statistical jargon, variability is the amount of imprecision and bias is the amount of inaccuracy. Precision is also called Positive Predictive Value (PPV).\n",
    "\n",
    "<div>\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/e/e7/Sensitivity_and_specificity.svg\" width=\"500\"/>\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/2/26/Precisionrecall.svg\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programming\n",
    "\n",
    "#### Python\n",
    "\n",
    "#### R\n",
    "\n",
    "#### SQL\n",
    "\n",
    "#### Spark/Hadoop/NoSQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supervised vs Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Explain the bias-variance tradeoff\n",
    "\n",
    "##### Tell me about an original algorithm youâ€™ve created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Explain Random Forest in layman's terms:\n",
    "\n",
    "Random Forest is a supervised machine learning algorithm that extends the CART paradigm. Decision trees are built in the conventional way, through recursive binary splitting; we take all of our data, and then split that data into two groups based on some characteristic or feature (M vs F). We then split those two child \"nodes\" on a different feature, and so on until some pre-determined depth. This would constitute one tree. When we decide how to split each node at each level of the tree, we have some criteria which determines the \"best\" split. However, this results in many similar trees if the process is repeated. To \"decorrelate\" these trees, we only consider a random sub-set of the available splitting features at each node of the tree. This is the \"random\" part of random forest. The forest part comes from repeating this random tree growing process many times, and aggregating the results of these trees to form the forest. To determine the outcome for unseen test data, we simply \"drop\" the new data down each tree in the forest. For classification, this results in the new data being classified into some category for each tree; we take the majority vote of all trees in the forest and use that to classify the new data. For regression, we average the output of each tree. It is an $\\mathit{ensemble}$ method, further improved by forcing trees to be grown differently so that a small subset of predictors don't dominate every tree. \n",
    "\n",
    "#### Explain Random Forest in technical terms/explain common split rules\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explain K-fold Cross Validation and why it is used\n",
    "\n",
    "The K in K-fold CV refers to the number of groups the training data is separated or \"folded\" into; for 5-fold CV we take the training data the divide it into 5 groups, at random. We then train our algorithm on 4 of the 5 groups, and use the left-out 5th group for testing or \"validation\" of the model. This means, in the case of 5-fold CV, that we will run our algorithm 5 times, with one fold held out each time. We then have 5 validation set error rates, which we average to obtain a predicted test set error rate. We use this validation error rate for model selection, by repeating the process with different tuning parameters. We choose the parameters based on lowest average validation error (or perhaps a variation), and then those parameters are used to train the model on the full training data. The benefit to doing this rather than simply using all training data for each combination of hyperparameters is that we reduce overfitting, because we are using the held out data for validation rather than simply using training set error rate for model selection. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General (Git/Linux/Data Science)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What are some ways to handle missing data?\n",
    "\n",
    "Missing data may be handled in the following ways:\n",
    "1. Impute missing values using some rule or function (median of values for that feature, for instance)\n",
    "2. Ignore missing values (usually not possible)\n",
    "3. Remove all observations which have any missing values/too many missing values\n",
    "4. Attempt to acquire missing data\n",
    "\n",
    "Validate by attempting multiple approaches. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Do you contribute to any open-source projects?\n",
    "\n",
    "##### Have you written a whitepaper?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
